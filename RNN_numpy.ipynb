{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf45glhEYLvD"
      },
      "source": [
        "# Building your Recurrent Neural Network in numpy\n",
        "Recurrent Neural Networks (RNN) are very effective for Natural Language Processing and other sequence tasks because they have \"memory\". They can read inputs $x^{\\langle t \\rangle}$ (such as words) one at a time, and remember some information/context through the hidden layer activations that get passed from one time-step to the next. This allows a uni-directional RNN to take information from the past to process later inputs. A bidirection RNN can take context from both the past and the future.\n",
        "\n",
        "**Notation**: \n",
        "- Superscript $[l]$ denotes an object associated with the $l^{th}$ layer. (eg. $a^{[4]}$ is the $4^{th}$ layer activation. $W^{[5]}$ and $b^{[5]}$ are the $5^{th}$ layer parameters.)\n",
        "- Superscript $(i)$ denotes an object associated with the $i^{th}$ example. (eg. $x^{(i)}$ is the $i^{th}$ training example input.)\n",
        "- Superscript $\\langle t \\rangle$ denotes an object at the $t^{th}$ time-step. (eg. $x^{\\langle t \\rangle}$ is the input x at the $t^{th}$ time-step. $x^{(i)\\langle t \\rangle}$ is the input at the $t^{th}$ timestep of example $i$.)\n",
        "- Lowerscript $i$ denotes the $i^{th}$ entry of a vector. (eg. $a^{[l]}_i$ denotes the $i^{th}$ entry of the activations in layer $l$.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDXGyMItYLvP"
      },
      "source": [
        "#### Packages and required util funciton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k9-rMYrVYbMR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def initialize_adam(parameters) :\n",
        "    \"\"\"\n",
        "    Initializes v and s as two python dictionaries with:\n",
        "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\"\n",
        "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters.\n",
        "                    parameters[\"W\" + str(l)] = Wl\n",
        "                    parameters[\"b\" + str(l)] = bl\n",
        "\n",
        "    Returns:\n",
        "    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n",
        "                    v[\"dW\" + str(l)] = ...\n",
        "                    v[\"db\" + str(l)] = ...\n",
        "    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n",
        "                    s[\"dW\" + str(l)] = ...\n",
        "                    s[\"db\" + str(l)] = ...\n",
        "\n",
        "    \"\"\"\n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "    v = {}\n",
        "    s = {}\n",
        "\n",
        "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
        "    for l in range(L):\n",
        "        v[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
        "        v[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
        "        s[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
        "        s[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
        "    return v, s\n",
        "\n",
        "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
        "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
        "    \"\"\"\n",
        "    Update parameters using Adam\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters:\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "    grads -- python dictionary containing your gradients for each parameters:\n",
        "                    grads['dW' + str(l)] = dWl\n",
        "                    grads['db' + str(l)] = dbl\n",
        "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
        "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
        "    learning_rate -- the learning rate, scalar.\n",
        "    beta1 -- Exponential decay hyperparameter for the first moment estimates\n",
        "    beta2 -- Exponential decay hyperparameter for the second moment estimates\n",
        "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters\n",
        "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
        "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
        "    \"\"\"\n",
        "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
        "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
        "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
        "\n",
        "    # Perform Adam update on all parameters\n",
        "    for l in range(L):\n",
        "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
        "        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1 - beta1) * grads[\"dW\" + str(l+1)]\n",
        "        v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1 - beta1) * grads[\"db\" + str(l+1)]\n",
        " \n",
        "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
        "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1 - beta1**t)\n",
        "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1 - beta1**t)\n",
        "        \n",
        "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
        "        s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1 - beta2) * (grads[\"dW\" + str(l+1)] ** 2)\n",
        "        s[\"db\" + str(l+1)] = beta2 * s[\"db\" + str(l+1)] + (1 - beta2) * (grads[\"db\" + str(l+1)] ** 2)\n",
        "        \n",
        "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
        "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / (1 - beta2 ** t)\n",
        "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1 - beta2 ** t)\n",
        "\n",
        "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v_corrected[\"dW\" + str(l+1)] / np.sqrt(s_corrected[\"dW\" + str(l+1)] + epsilon)\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v_corrected[\"db\" + str(l+1)] / np.sqrt(s_corrected[\"db\" + str(l+1)] + epsilon)\n",
        "\n",
        "    return parameters, v, s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6-1eA0ZYLvQ"
      },
      "source": [
        "## 1 - Forward propagation for the basic Recurrent Neural Network \n",
        "*In this example, $T_x = T_y$.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME4jBsCgYLvQ"
      },
      "source": [
        "<img src=\"https://github.com/QuantumEverything/QuantumEverything/assets/53507106/b205eb74-ccf1-47bd-96a4-96c7a66a8644\" style=\"width:540;height:250px;\">   \n",
        "<br>\n",
        "<caption>**Figure 1**: Basic RNN model</caption>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKyyz9QRYLvQ"
      },
      "source": [
        "**Steps**:\n",
        "1. Implement the calculations needed for one time-step of the RNN.\n",
        "2. Implement a loop over $T_x$ time-steps in order to process all the inputs, one at a time.\n",
        "\n",
        "## 1.1 - RNN cell\n",
        "A Recurrent neural network can be seen as the repetition of a single cell. Firstly, going to implement the computations for a single time-step. The following figure describes the operations for a single time-step of an RNN cell.\n",
        "\n",
        "<img src=\"https://github.com/QuantumEverything/QuantumEverything/assets/53507106/25f1de17-ce4a-49ea-8f6d-ef5b6b8ad790\" style=\"width:700px;height:300px;\"><br>\n",
        "**Figure 2**: Basic RNN cell.<br> \n",
        "Takes as input $x^{\\langle t \\rangle}$ (current input) and $a^{\\langle t - 1\\rangle}$ (previous hidden state containing information from the past), and outputs $a^{\\langle t \\rangle}$ which is given to the next RNN cell and also used to predict $y^{\\langle t \\rangle}$ \n",
        "\n",
        "**Implement the RNN-cell described in Figure (2)**\n",
        "1. Compute the hidden state with **tanh activation**:   $a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a)$.\n",
        "2. Using your new hidden state $a^{\\langle t \\rangle}$, compute the prediction $\\hat{y}^{\\langle t \\rangle} = softmax(W_{ya} a^{\\langle t \\rangle} + b_y)$. We provided you a function: `softmax`.\n",
        "3. Store $(a^{\\langle t \\rangle}, a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}, parameters)$ in cache\n",
        "4. Return $a^{\\langle t \\rangle}$ , $y^{\\langle t \\rangle}$ and cache\n",
        "\n",
        "We will vectorize over $m$ examples. Thus, $x^{\\langle t \\rangle}$ will have dimension $(n_x,m)$, and $a^{\\langle t \\rangle}$ will have dimension $(n_a,m)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "b-7qlly-YLvQ"
      },
      "outputs": [],
      "source": [
        "def rnn_cell_forward(xt, a_prev, parameters):\n",
        "    \"\"\"\n",
        "    Implements a single forward step of the RNN-cell as described in Figure (2)\n",
        "\n",
        "    Arguments:\n",
        "    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
        "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
        "    parameters -- python dictionary containing:\n",
        "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
        "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
        "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        ba --  Bias, numpy array of shape (n_a, 1)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "    Returns:\n",
        "    a_next -- next hidden state, of shape (n_a, m)\n",
        "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
        "    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve parameters from \"parameters\"\n",
        "    Wax = parameters[\"Wax\"]\n",
        "    Waa = parameters[\"Waa\"]\n",
        "    Wya = parameters[\"Wya\"]\n",
        "    ba = parameters[\"ba\"]\n",
        "    by = parameters[\"by\"]\n",
        "    \n",
        "    # compute next activation state using the formula given above\n",
        "    a_next = np.tanh(np.dot(Wax, xt) + np.dot(Waa, a_prev) + ba)\n",
        "    # compute output of the current cell using the formula given above\n",
        "    yt_pred = softmax(np.dot(Wya, a_next) + by)\n",
        "\n",
        "    # store values you need for backward propagation in cache\n",
        "    cache = (a_next, a_prev, xt, parameters)\n",
        "\n",
        "    return a_next, yt_pred, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZ3hBytfYLvR",
        "outputId": "e533a487-9808-4161-c5bf-e4dba0bd628a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a_next[4] =  [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n",
            " -0.18887155  0.99815551  0.6531151   0.82872037]\n",
            "a_next.shape =  (5, 10)\n",
            "yt_pred[1] = [0.9888161  0.01682021 0.21140899 0.36817467 0.98988387 0.88945212\n",
            " 0.36920224 0.9966312  0.9982559  0.17746526]\n",
            "yt_pred.shape =  (2, 10)\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(1)\n",
        "xt = np.random.randn(3,10)\n",
        "a_prev = np.random.randn(5,10)\n",
        "Waa = np.random.randn(5,5)\n",
        "Wax = np.random.randn(5,3)\n",
        "Wya = np.random.randn(2,5)\n",
        "ba = np.random.randn(5,1)\n",
        "by = np.random.randn(2,1)\n",
        "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
        "\n",
        "a_next, yt_pred, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
        "print(\"a_next[4] = \", a_next[4])\n",
        "print(\"a_next.shape = \", a_next.shape)\n",
        "print(\"yt_pred[1] =\", yt_pred[1])\n",
        "print(\"yt_pred.shape = \", yt_pred.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 - RNN forward pass\n",
        "\n",
        "Can see an RNN as the repetition of the cell you've just built. If input sequence of data is carried over 10 time steps, then will copy the RNN cell 10 times. Each cell takes as input the hidden state from the previous cell ($a^{\\langle t-1 \\rangle}$) and the current time-step's input data ($x^{\\langle t \\rangle}$). It outputs a hidden state ($a^{\\langle t \\rangle}$) and a prediction ($y^{\\langle t \\rangle}$) for this time-step.\n",
        "\n",
        "<img src=\"https://github.com/QuantumEverything/QuantumEverything/assets/53507106/42b8d75d-1f49-447f-a7dc-245d897f49ff\" style=\"width:800px;height:300px;\"><br> \n",
        "Figure 3: Basic RNN<br> \n",
        "The input sequence $x = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$  is carried over $T_x$ time steps. The network outputs $y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$. \n",
        "\n",
        "**Code the forward propagation of the RNN described in Figure (3)**\n",
        "1. Create a vector of zeros ($a$) that will store all the hidden states computed by the RNN.\n",
        "2. Initialize the \"next\" hidden state as $a_0$ (initial hidden state).\n",
        "3. Start looping over each time step, your incremental index is $t$ :\n",
        "    - Update the \"next\" hidden state and the cache by running `rnn_cell_forward`\n",
        "    - Store the \"next\" hidden state in $a$ ($t^{th}$ position)\n",
        "    - Store the prediction in y\n",
        "    - Add the cache to the list of caches\n",
        "4. Return $a$, $y$ and caches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "i9kd_PRjYLvS"
      },
      "outputs": [],
      "source": [
        "def rnn_forward(x, a0, parameters):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation of the recurrent neural network described in Figure (3).\n",
        "\n",
        "    Arguments:\n",
        "    x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
        "    a0 -- Initial hidden state, of shape (n_a, m)\n",
        "    parameters -- python dictionary containing:\n",
        "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
        "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
        "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        ba --  Bias numpy array of shape (n_a, 1)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "\n",
        "    Returns:\n",
        "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
        "    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
        "    caches -- tuple of values needed for the backward pass, contains (list of caches, x)\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize \"caches\" which will contain the list of all caches\n",
        "    caches = []\n",
        "\n",
        "    # Retrieve dimensions from shapes of x and Wy\n",
        "    n_x, m, T_x = x.shape\n",
        "    n_y, n_a = parameters[\"Wya\"].shape\n",
        "\n",
        "    # initialize \"a\" and \"y\" with zeros (≈2 lines)\n",
        "    a = np.zeros((n_a, m, T_x))\n",
        "    y_pred = np.zeros((n_y, m, T_x))\n",
        "\n",
        "    # Initialize a_next (≈1 line)\n",
        "    a_next = a0\n",
        "\n",
        "    # loop over all time-steps\n",
        "    for t in range(T_x):\n",
        "        # Update next hidden state, compute the prediction, get the cache (≈1 line)\n",
        "        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a_next, parameters)\n",
        "        # Save the value of the new \"next\" hidden state in a (≈1 line)\n",
        "        a[:,:,t] = a_next\n",
        "        # Save the value of the prediction in y (≈1 line)\n",
        "        y_pred[:,:,t] = yt_pred\n",
        "        # Append \"cache\" to \"caches\" (≈1 line)\n",
        "        caches.append(cache)\n",
        "\n",
        "    # store values needed for backward propagation in cache\n",
        "    caches = (caches, x)\n",
        "\n",
        "    return a, y_pred, caches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYsKJZj3YLvS",
        "outputId": "c8066121-4a83-4968-de95-8059958c9e7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a[4][1] =  [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n",
            "a.shape =  (5, 10, 4)\n",
            "y_pred[1][3] = [0.79560373 0.86224861 0.11118257 0.81515947]\n",
            "y_pred.shape =  (2, 10, 4)\n",
            "caches[1][1][3] = [-1.1425182  -0.34934272 -0.20889423  0.58662319]\n",
            "len(caches) =  2\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(1)\n",
        "x = np.random.randn(3,10,4)\n",
        "a0 = np.random.randn(5,10)\n",
        "Waa = np.random.randn(5,5)\n",
        "Wax = np.random.randn(5,3)\n",
        "Wya = np.random.randn(2,5)\n",
        "ba = np.random.randn(5,1)\n",
        "by = np.random.randn(2,1)\n",
        "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
        "\n",
        "a, y_pred, caches = rnn_forward(x, a0, parameters)\n",
        "print(\"a[4][1] = \", a[4][1])\n",
        "print(\"a.shape = \", a.shape)\n",
        "print(\"y_pred[1][3] =\", y_pred[1][3])\n",
        "print(\"y_pred.shape = \", y_pred.shape)\n",
        "print(\"caches[1][1][3] =\", caches[1][1][3])\n",
        "print(\"len(caches) = \", len(caches))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egyaA7PhYLvS"
      },
      "source": [
        "Built the forward propagation of a recurrent neural network from scratch. This will work well enough for some applications, but it suffers from vanishing gradient problems. So it works best when each output $y^{\\langle t \\rangle}$ can be estimated using mainly \"local\" context (meaning information from inputs $x^{\\langle t' \\rangle}$ where $t'$ is not too far from $t$).\n",
        "\n",
        "In the next part, you will build a more complex LSTM model, which is better at addressing vanishing gradients. The LSTM will be better able to remember a piece of information and keep it saved for many timesteps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Backpropagation in Recurrent Neural Networks \n",
        "Note: This notebook does not implement the backward path from the Loss 'J' backwards to 'a'. This would have included the dense layer and softmax, which are a part of the forward path. This is assumed to be calculated elsewhere and the result passed to rnn_backward in 'da'. It is further assumed that loss has been adjusted for batch size (m) and division by the number of examples is NOT required here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 - Basic RNN Backward Pass\n",
        "Begin by computing the backward pass for the basic RNN cell. Then, in the following sections, iterate through the cells.\n",
        "<br><br>\n",
        "![Screenshot 2023-12-16 at 2 34 21 PM](https://github.com/QuantumEverything/QuantumEverything/assets/53507106/c130e7f3-c983-4f1d-8439-93531c2341e7)\n",
        "<br>Figure 4: The RNN cell's backward pass. Just like in a fully-connected neural network, the derivative of the cost function $J$ backpropagates through the time steps of the RNN by following the chain rule from calculus. Internal to the cell, the chain rule is also used to calculate $(\\frac{\\partial J}{\\partial W_{ax}},\\frac{\\partial J}{\\partial W_{aa}},\\frac{\\partial J}{\\partial b})$ to update the parameters $(W_{ax}, W_{aa}, b_a)$. The operation can utilize the cached results from the forward path.\n",
        "\n",
        "The shorthand for the partial derivative of cost relative to a variable is `dVariable`. For example, $\\frac{\\partial J}{\\partial W_{ax}}$ is $dW_{ax}$. This will be used throughout the remaining sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Screenshot 2023-12-16 at 3 02 28 PM](https://github.com/QuantumEverything/QuantumEverything/assets/53507106/c64658f3-93ae-4610-a8f1-2a6e3a750a25)<br>Figure 5: This implementation of `rnn_cell_backward` does **not** include the output dense layer and softmax which are included in `rnn_cell_forward`.  \n",
        "$da_{next}$ is $\\frac{\\partial{J}}{\\partial a^{\\langle t \\rangle}}$ and includes loss from previous stages and current stage output logic. The addition shown in green will be part of your implementation of `rnn_backward`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Equations\n",
        "The following equationson is for computing `rnn_cell_backward`. <br>\n",
        "$*$ denotes element-wise multiplication while the absence of a symbol indicates matrix multiplication.<br>\n",
        "\\begin{align}\n",
        "\\displaystyle a^{\\langle t \\rangle} &= \\tanh(W_{ax} x^{\\langle t \\rangle} + W_{aa} a^{\\langle t-1 \\rangle} + b_{a})\\tag{-} \\\\[8pt]\n",
        "\\displaystyle \\frac{\\partial \\tanh(x)} {\\partial x} &= 1 - \\tanh^2(x) \\tag{-} \\\\[8pt]\n",
        "\\displaystyle {dtanh} &= da_{next} * ( 1 - \\tanh^2(W_{ax}x^{\\langle t \\rangle}+W_{aa} a^{\\langle t-1 \\rangle} + b_{a})) \\tag{0} \\\\[8pt]\n",
        "\\displaystyle  {dW_{ax}} &= dtanh \\cdot x^{\\langle t \\rangle T}\\tag{1} \\\\[8pt]\n",
        "\\displaystyle dW_{aa} &= dtanh \\cdot a^{\\langle t-1 \\rangle T}\\tag{2} \\\\[8pt]\n",
        "\\displaystyle db_a& = \\sum_{batch}dtanh\\tag{3} \\\\[8pt]\n",
        "\\displaystyle dx^{\\langle t \\rangle} &= { W_{ax}}^T \\cdot dtanh\\tag{4} \\\\[8pt]\n",
        "\\displaystyle da_{prev} &= { W_{aa}}^T \\cdot dtanh\\tag{5}\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='ex-5'></a>\n",
        "### 2.2 rnn_cell_backward\n",
        "\n",
        "The results can be computed directly by implementing the equations above. However, you have an option to simplify them by computing 'dz' and using the chain rule.  \n",
        "This can be further simplified by noting that $\\tanh(W_{ax}x^{\\langle t \\rangle}+W_{aa} a^{\\langle t-1 \\rangle} + b_{a})$ was computed and saved as `a_next` in the forward pass. \n",
        "\n",
        "To calculate `dba`, the 'batch' above is a sum across all 'm' examples (axis= 1). Note that you should use the `keepdims = True` option.\n",
        "\n",
        "It may be worthwhile to review Course 1 [Derivatives with a Computation Graph](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/0VSHe/derivatives-with-a-computation-graph)  through  [Backpropagation Intuition](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/6dDj7/backpropagation-intuition-optional), which decompose the calculation into steps using the chain rule.  \n",
        "Matrix vector derivatives are described [here](http://cs231n.stanford.edu/vecDerivs.pdf), though the equations above incorporate the required transformations.\n",
        "\n",
        "**Note**: `rnn_cell_backward` does **not** include the calculation of loss from $y \\langle t \\rangle$. This is incorporated into the incoming `da_next`. This is a slight mismatch with `rnn_cell_forward`, which includes a dense layer and softmax. \n",
        "\n",
        "**Note on the code**: \n",
        "  \n",
        "$\\displaystyle dx^{\\langle t \\rangle}$ is represented by dxt,   \n",
        "$\\displaystyle d W_{ax}$ is represented by dWax,   \n",
        "$\\displaystyle da_{prev}$ is represented by da_prev,    \n",
        "$\\displaystyle dW_{aa}$ is represented by dWaa,   \n",
        "$\\displaystyle db_{a}$ is represented by dba,   \n",
        "`dz` is not derived above but can optionally be derived by students to simplify the repeated calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rnn_cell_backward(da_next, cache):\n",
        "    \"\"\"\n",
        "    Implements the backward pass for the RNN-cell (single time-step).\n",
        "\n",
        "    Arguments:\n",
        "    da_next -- Gradient of loss with respect to next hidden state\n",
        "    cache -- python dictionary containing useful values (output of rnn_cell_forward())\n",
        "\n",
        "    Returns:\n",
        "    gradients -- python dictionary containing:\n",
        "                        dx -- Gradients of input data, of shape (n_x, m)\n",
        "                        da_prev -- Gradients of previous hidden state, of shape (n_a, m)\n",
        "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
        "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
        "                        dba -- Gradients of bias vector, of shape (n_a, 1)\n",
        "    \"\"\"\n",
        "    # Retrieve values from cache\n",
        "    (a_next, a_prev, xt, parameters) = cache\n",
        "    \n",
        "    # Retrieve values from parameters\n",
        "    Wax = parameters[\"Wax\"]\n",
        "    Waa = parameters[\"Waa\"]\n",
        "    Wya = parameters[\"Wya\"]\n",
        "    ba = parameters[\"ba\"]\n",
        "    by = parameters[\"by\"]\n",
        "\n",
        "    # compute the gradient of tanh with respect to a_next (≈1 line)\n",
        "    dtanh = (1-a_next*a_next)*da_next\n",
        "\n",
        "    # compute the gradient of the loss with respect to Wax (≈2 lines)\n",
        "    dxt = np.dot(Wax.T, dtanh)\n",
        "    dWax = np.dot(dtanh, xt.T)\n",
        "\n",
        "    # compute the gradient with respect to Waa (≈2 lines)\n",
        "    da_prev = np.dot(Waa.T, dtanh)  \n",
        "    dWaa = np.dot( dtanh,a_prev.T)\n",
        "\n",
        "    # compute the gradient with respect to b (≈1 line)\n",
        "    dba = np.sum( dtanh,keepdims=True,axis=-1)\n",
        "\n",
        "    # Store the gradients in a python dictionary\n",
        "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dWax\": dWax, \"dWaa\": dWaa, \"dba\": dba}\n",
        "    \n",
        "    return gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(1)\n",
        "xt_tmp = np.random.randn(3,10)\n",
        "a_prev_tmp = np.random.randn(5,10)\n",
        "parameters_tmp = {}\n",
        "parameters_tmp['Wax'] = np.random.randn(5,3)\n",
        "parameters_tmp['Waa'] = np.random.randn(5,5)\n",
        "parameters_tmp['Wya'] = np.random.randn(2,5)\n",
        "parameters_tmp['ba'] = np.random.randn(5,1)\n",
        "parameters_tmp['by'] = np.random.randn(2,1)\n",
        "\n",
        "a_next_tmp, yt_tmp, cache_tmp = rnn_cell_forward(xt_tmp, a_prev_tmp, parameters_tmp)\n",
        "\n",
        "da_next_tmp = np.random.randn(5,10)\n",
        "gradients_tmp = rnn_cell_backward(da_next_tmp, cache_tmp)\n",
        "print(\"gradients[\\\"dxt\\\"][1][2] =\", gradients_tmp[\"dxt\"][1][2])\n",
        "print(\"gradients[\\\"dxt\\\"].shape =\", gradients_tmp[\"dxt\"].shape)\n",
        "print(\"gradients[\\\"da_prev\\\"][2][3] =\", gradients_tmp[\"da_prev\"][2][3])\n",
        "print(\"gradients[\\\"da_prev\\\"].shape =\", gradients_tmp[\"da_prev\"].shape)\n",
        "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients_tmp[\"dWax\"][3][1])\n",
        "print(\"gradients[\\\"dWax\\\"].shape =\", gradients_tmp[\"dWax\"].shape)\n",
        "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients_tmp[\"dWaa\"][1][2])\n",
        "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients_tmp[\"dWaa\"].shape)\n",
        "print(\"gradients[\\\"dba\\\"][4] =\", gradients_tmp[\"dba\"][4])\n",
        "print(\"gradients[\\\"dba\\\"].shape =\", gradients_tmp[\"dba\"].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 rnn_backward\n",
        "\n",
        "Computing the gradients of the cost with respect to $a^{\\langle t \\rangle}$ at every time step $t$ is useful because it is what helps the gradient backpropagate to the previous RNN cell. To do so, you need to iterate through all the time steps starting at the end, and at each step, you increment the overall $db_a$, $dW_{aa}$, $dW_{ax}$ and you store $dx$.\n",
        "\n",
        "Implement the `rnn_backward` function:  \n",
        "Initialize the return variables with zeros first, and then loop through all the time steps while calling the `rnn_cell_backward` at each time time step, updating the other variables accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rnn_backward(da, caches):\n",
        "    \"\"\"\n",
        "    Implement the backward pass for a RNN over an entire sequence of input data.\n",
        "\n",
        "    Arguments:\n",
        "    da -- Upstream gradients of all hidden states, of shape (n_a, m, T_x)\n",
        "    caches -- tuple containing information from the forward pass (rnn_forward)\n",
        "    \n",
        "    Returns:\n",
        "    gradients -- python dictionary containing:\n",
        "                        dx -- Gradient w.r.t. the input data, numpy-array of shape (n_x, m, T_x)\n",
        "                        da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (n_a, m)\n",
        "                        dWax -- Gradient w.r.t the input's weight matrix, numpy-array of shape (n_a, n_x)\n",
        "                        dWaa -- Gradient w.r.t the hidden state's weight matrix, numpy array of shape (n_a, n_a)\n",
        "                        dba -- Gradient w.r.t the bias, of shape (n_a, 1)\n",
        "    \"\"\"\n",
        "    # Retrieve values from the first cache (t=1) of caches (≈2 lines)\n",
        "    (caches, x) = caches\n",
        "    (a1, a0, x1, parameters) = caches[0]\n",
        "    \n",
        "    # Retrieve dimensions from da's and x1's shapes (≈2 lines)\n",
        "    n_a, m, T_x = da.shape\n",
        "    n_x, m = x1.shape \n",
        "    \n",
        "    # initialize the gradients with the right sizes (≈6 lines)\n",
        "    dx = np.zeros((n_x, m, T_x)) \n",
        "    dWax = np.zeros((n_a, n_x))\n",
        "    dWaa = np.zeros((n_a, n_a))\n",
        "    dba = np.zeros((n_a, 1)) \n",
        "    da0 = np.zeros((n_a, m))\n",
        "    da_prevt = np.zeros((n_a, m))  \n",
        "    \n",
        "    # Loop through all the time steps\n",
        "    for t in reversed(range(T_x)):\n",
        "        # Compute gradients at time step t. Choose wisely the \"da_next\" and the \"cache\" to use in the backward propagation step. (≈1 line)\n",
        "        gradients = rnn_cell_backward(da[:, :, t] + da_prevt, caches[t])\n",
        "        # Retrieve derivatives from gradients (≈ 1 line)\n",
        "        dxt, da_prevt, dWaxt, dWaat, dbat = gradients[\"dxt\"], gradients[\"da_prev\"], gradients[\"dWax\"], gradients[\"dWaa\"], gradients[\"dba\"]\n",
        "        # Increment global derivatives w.r.t parameters by adding their derivative at time-step t (≈4 lines)\n",
        "        dx[:, :, t] = dxt  \n",
        "        dWax += dWaxt  \n",
        "        dWaa += dWaat  \n",
        "        dba += dbat  \n",
        "        \n",
        "    # Set da0 to the gradient of a which has been backpropagated through all time-steps (≈1 line) \n",
        "    da0 = da_prevt\n",
        "    \n",
        "    # Store the gradients in a python dictionary\n",
        "    gradients = {\"dx\": dx, \"da0\": da0, \"dWax\": dWax, \"dWaa\": dWaa,\"dba\": dba}\n",
        "    \n",
        "    return gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(1)\n",
        "x_tmp = np.random.randn(3,10,4)\n",
        "a0_tmp = np.random.randn(5,10)\n",
        "parameters_tmp = {}\n",
        "parameters_tmp['Wax'] = np.random.randn(5,3)\n",
        "parameters_tmp['Waa'] = np.random.randn(5,5)\n",
        "parameters_tmp['Wya'] = np.random.randn(2,5)\n",
        "parameters_tmp['ba'] = np.random.randn(5,1)\n",
        "parameters_tmp['by'] = np.random.randn(2,1)\n",
        "\n",
        "a_tmp, y_tmp, caches_tmp = rnn_forward(x_tmp, a0_tmp, parameters_tmp)\n",
        "da_tmp = np.random.randn(5, 10, 4)\n",
        "gradients_tmp = rnn_backward(da_tmp, caches_tmp)\n",
        "\n",
        "print(\"gradients[\\\"dx\\\"][1][2] =\", gradients_tmp[\"dx\"][1][2])\n",
        "print(\"gradients[\\\"dx\\\"].shape =\", gradients_tmp[\"dx\"].shape)\n",
        "print(\"gradients[\\\"da0\\\"][2][3] =\", gradients_tmp[\"da0\"][2][3])\n",
        "print(\"gradients[\\\"da0\\\"].shape =\", gradients_tmp[\"da0\"].shape)\n",
        "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients_tmp[\"dWax\"][3][1])\n",
        "print(\"gradients[\\\"dWax\\\"].shape =\", gradients_tmp[\"dWax\"].shape)\n",
        "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients_tmp[\"dWaa\"][1][2])\n",
        "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients_tmp[\"dWaa\"].shape)\n",
        "print(\"gradients[\\\"dba\\\"][4] =\", gradients_tmp[\"dba\"][4])\n",
        "print(\"gradients[\\\"dba\\\"].shape =\", gradients_tmp[\"dba\"].shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "coursera": {
      "course_slug": "nlp-sequence-models",
      "graded_item_id": "xxuVc",
      "launcher_item_id": "X20PE"
    },
    "kernelspec": {
      "display_name": "tf29_py39",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
